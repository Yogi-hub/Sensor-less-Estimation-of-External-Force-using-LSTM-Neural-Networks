{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import scipy.io as scio\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Torch cuda is available: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data processing\n",
    "Before running this notebook, the Matlab script in `./data/adataprocessing.m` should be executed first.\n",
    "\n",
    "`./data/adataprocessing.m` will transform the simulation results in `./data/raw_data/` into `./data/raw_dataset.mat`, which will be used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "output_size = 1\n",
    "batch_size = 64\n",
    "sequence_length = 100\n",
    "train_dataset_rate = 0.8\n",
    "validation_dataset_rate = 0.1\n",
    "model_name = \"model_20240525_only_force_case3_hyperpara_search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=scio.loadmat('./data/raw_dataset.mat')\n",
    "\n",
    "current_data = data['current_data']\n",
    "force_data = data['force_data']\n",
    "voltage_data = data['voltage_data']\n",
    "print(\"raw current shape: \",current_data.shape)\n",
    "print(\"raw voltage shape: \",voltage_data.shape)\n",
    "print(\"raw force shape: \",force_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_len = int(current_data.shape[1]/sequence_length)*sequence_length\n",
    "sample_len = current_data.shape[1]\n",
    "print(\"For one sample,\",split_data_len,'/',sample_len, \"time steps are used.\")\n",
    "current_tensor = torch.tensor(current_data[:,current_data.shape[1]-split_data_len : current_data.shape[1]].reshape(-1,))\n",
    "voltage_tensor = torch.tensor(voltage_data[:,current_data.shape[1]-split_data_len : current_data.shape[1]].reshape(-1,))\n",
    "force_tensor = torch.tensor(force_data[:,current_data.shape[1]-split_data_len : current_data.shape[1]].reshape(-1,))\n",
    "\n",
    "print(\"current_tensor size: \",current_tensor.shape)\n",
    "print(\"voltage_tensor size: \",voltage_tensor.shape)\n",
    "print(\"force_tensor size: \",force_tensor.shape)\n",
    "\n",
    "x_raw_data = torch.stack((current_tensor, voltage_tensor), dim=1).float().reshape(-1, sequence_length, input_size)\n",
    "print(\"\\nx_raw_data size: \", x_raw_data.shape)\n",
    "y_raw_data = force_tensor.float().reshape(-1, sequence_length, output_size).mean(dim=1)\n",
    "print(\"y_raw_data size: \", y_raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = x_raw_data.reshape(-1,input_size).mean(dim=0)\n",
    "x_std = x_raw_data.reshape(-1,input_size).std(dim=0)\n",
    "y_mean = y_raw_data.reshape(-1,output_size).mean(dim=0)\n",
    "y_std = y_raw_data.reshape(-1,output_size).std(dim=0)\n",
    "\n",
    "print(\"For raw data:\")\n",
    "print(\"\\tmean of x: \",x_mean)\n",
    "print(\"\\tstd of x: \",x_std)\n",
    "print(\"\\tmean of y: \",y_mean)\n",
    "print(\"\\tstd of y: \",y_std)\n",
    "\n",
    "\n",
    "x_norm_data = ((x_raw_data.reshape(-1,input_size)-x_mean)/x_std).reshape(-1, sequence_length, input_size)\n",
    "y_norm_data = ((y_raw_data.reshape(-1,output_size)-y_mean)/y_std).reshape(-1, output_size)\n",
    "print(\"\\nx_norm_data size: \", x_norm_data.shape)\n",
    "print(\"y_norm_data size: \", y_norm_data.shape)\n",
    "\n",
    "print(\"For normalized data:\")\n",
    "print(\"\\tmean of x: \",x_norm_data.reshape(-1,input_size).mean(dim=0))\n",
    "print(\"\\tstd of x: \",x_norm_data.reshape(-1,input_size).std(dim=0))\n",
    "print(\"\\tmean of y: \",y_norm_data.reshape(-1,output_size).mean(dim=0))\n",
    "print(\"\\tstd of y: \",y_norm_data.reshape(-1,output_size).std(dim=0))\n",
    "\n",
    "norm_dataset = TensorDataset(x_norm_data, y_norm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(train_dataset_rate * x_raw_data.shape[0])\n",
    "test_size = x_raw_data.shape[0] - train_size\n",
    "train_valid_dataset, test_data = random_split(norm_dataset, [train_size, test_size])\n",
    "\n",
    "val_size = int(validation_dataset_rate * len(train_valid_dataset))\n",
    "train_size = len(train_valid_dataset) - val_size\n",
    "train_data, val_data = random_split(train_valid_dataset, [train_size, val_size])\n",
    "\n",
    "print(\"train_dataset size: \",len(train_data))\n",
    "print(\"validation_dataset size: \",len(val_data))\n",
    "print(\"test_dataset size: \",len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Save data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./models/\" + model_name + '/running_vars/'\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "torch.save(train_data,dir + \"train_data.pt\")\n",
    "torch.save(val_data,dir + \"validation_data.pt\")\n",
    "torch.save(test_data,dir + \"test_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data shape\n",
    "for inputs, labels in train_loader:\n",
    "    print(inputs.shape)\n",
    "    print(inputs.dtype)\n",
    "    print(labels.shape)\n",
    "    print(labels.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=50, num_layers=1):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=50, num_layers=1, output_size=50):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        output = hidden[-1, :,:]\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.encoder = LSTMEncoder(input_size, hidden_size, num_layers)\n",
    "        self.decoder = LSTMDecoder(input_size, hidden_size, num_layers, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden, cell = self.encoder(x)\n",
    "        decoded = self.decoder(x, hidden, cell)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Hyper-parameter searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size_search = list(range(80,161,24))\n",
    "num_layers_search = list(range(1,4,1))\n",
    "learning_rate_search = np.linspace(0.001, 0.003, 2)\n",
    "search_loss = np.zeros((len(hidden_size_search),\n",
    "                        len(num_layers_search),\n",
    "                        len(learning_rate_search)))\n",
    "num_loops = len(hidden_size_search)*len(num_layers_search)*len(learning_rate_search)\n",
    "with tqdm(total=num_loops) as pbar:\n",
    "    for hi, hidden_size in enumerate(hidden_size_search):\n",
    "        for ni, num_layers in enumerate(num_layers_search):\n",
    "            for li, learning_rate in enumerate(learning_rate_search):\n",
    "                model = LSTMAutoencoder(input_size, hidden_size, num_layers, output_size)\n",
    "                loss_function = nn.MSELoss()\n",
    "                if torch.cuda.is_available():\n",
    "                    model = model.cuda()\n",
    "                    loss_function = loss_function.cuda()\n",
    "\n",
    "                optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                num_epochs = 10\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.train()\n",
    "                    for inputs, targets in train_loader:\n",
    "                        if torch.cuda.is_available():\n",
    "                            inputs = inputs.cuda()\n",
    "                            targets = targets.cuda()\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(inputs)\n",
    "                        loss = loss_function(output, targets) \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                model.eval()\n",
    "                test_loss = 0.0\n",
    "                with torch.no_grad(): \n",
    "                    for inputs, targets in test_loader:\n",
    "                        if torch.cuda.is_available():\n",
    "                            inputs = inputs.cuda()\n",
    "                            targets = targets.cuda()\n",
    "                        output = model(inputs)\n",
    "                        loss = loss_function(output, targets) \n",
    "                        test_loss += loss.item() * inputs.size(0)\n",
    "                search_loss[hi,ni,li] = test_loss / len(test_loader.dataset)\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = np.min(np.nan_to_num(search_loss,nan=np.sum(np.nan_to_num(search_loss,nan=1))))\n",
    "print(f\"Minimum loss: {min_loss}\")\n",
    "\n",
    "min_index_flat = np.argmin(search_loss)\n",
    "min_index_3d = np.unravel_index(min_index_flat, search_loss.shape)\n",
    "print(f\"Index of the minimum loss: {min_index_3d}\")\n",
    "\n",
    "print(f\"Best parameters:\\\n",
    "      \\n\\thidden state = {hidden_size_search[min_index_3d[0]]}\\\n",
    "      \\n\\thidden layers = {num_layers_search[min_index_3d[1]]}\\\n",
    "      \\n\\tlearning rate = { learning_rate_search[min_index_3d[2]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = hidden_size_search\n",
    "param2 = num_layers_search\n",
    "results = search_loss[:,:,min_index_3d[2]].T\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.arange(len(param1))\n",
    "y = np.arange(len(param2))\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "ax.plot_wireframe(x, y, results, color='b')\n",
    "ax.scatter(min_index_3d[0], min_index_3d[1], min_loss, color='r', s=50, label=f'Min Value')\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(len(param1)))\n",
    "ax.set_xticklabels(hidden_size_search)\n",
    "ax.set_yticks(np.arange(len(param2)))\n",
    "ax.set_yticklabels(num_layers_search)\n",
    "ax.set_xlabel('Hidden States size')\n",
    "ax.set_ylabel('Hidden Layers size')\n",
    "ax.set_zlabel('Loss')\n",
    "ax.set_title(f'Loss Surface for Different Hidden States and Layers \\n(Learning Rate = {learning_rate_search[min_index_3d[2]]})')\n",
    "# plt.subplots_adjust(left=0.0, right=1, top=0.9, bottom=0.1)\n",
    "ax.legend()\n",
    "\n",
    "dir = \"./models/\"+ model_name +\"/results/\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "plt.savefig(dir+\"hyper_parameter.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Choose best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = np.min(search_loss)\n",
    "min_index_flat = np.argmin(search_loss)\n",
    "min_index_3d = np.unravel_index(min_index_flat, search_loss.shape)\n",
    "\n",
    "hidden_size = hidden_size_search[min_index_3d[0]]\n",
    "num_layers = num_layers_search[min_index_3d[1]]\n",
    "learning_rate = learning_rate_search[min_index_3d[2]]\n",
    "\n",
    "model = LSTMAutoencoder(input_size, hidden_size, num_layers, output_size)\n",
    "loss_function = nn.MSELoss()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    loss_function = loss_function.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "dir = \"./models/\"+ model_name +\"/results/\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "torch.save(hidden_size,dir + \"hidden_size.pt\")\n",
    "torch.save(num_layers,dir + \"num_layers.pt\")\n",
    "torch.save(learning_rate,dir + \"learning_rate.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "train_loss_epoch = np.zeros(num_epochs)\n",
    "val_loss_epoch = np.zeros(num_epochs)\n",
    "dir = \"./models/\" + model_name + '/epochs/'\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "for epoch in trange(num_epochs):\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"input shape:\", inputs.shape)\n",
    "        # print(\"target shape:\", targets.shape)\n",
    "        output = model(inputs)\n",
    "        # print(\"output shape:\", output.shape)\n",
    "        loss = loss_function(output, targets) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss_epoch[epoch] = train_loss/len(train_loader)\n",
    "    \n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): \n",
    "        for inputs, targets in val_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "            output = model(inputs)\n",
    "            loss = loss_function(output, targets) \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "        val_loss_epoch[epoch] = val_loss/len(val_loader)\n",
    "    \n",
    "    torch.save(model.state_dict(), dir + \"model_\"+str(epoch+1)+\".pth\")\n",
    "    \n",
    "plt.plot(train_loss_epoch)\n",
    "plt.plot(val_loss_epoch)\n",
    "plt.legend([\"training loss\",\"validation loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.ylabel(\"Loss\")\n",
    "dir = \"./models/\"+ model_name +\"/results/\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "plt.savefig(dir+\"loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./models/\"+ model_name +\"/results/\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "torch.save(train_loss_epoch,dir + \"training_loss.pt\")\n",
    "torch.save(val_loss_epoch,dir + \"validation_loss.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "selected_model = num_epochs #the last model\n",
    "dir = \"./models/\" + model_name + '/epochs/' + \"model_\"+str(num_epochs) +\".pth\"\n",
    "model = LSTMAutoencoder(input_size, hidden_size, num_layers, output_size)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "model.load_state_dict(torch.load(dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_output = torch.tensor([])\n",
    "test_target = torch.tensor([])\n",
    "if torch.cuda.is_available():\n",
    "    test_output = test_output.cuda()\n",
    "    test_target = test_target.cuda()\n",
    "with torch.no_grad(): \n",
    "    for inputs, targets in test_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        output = model(inputs)\n",
    "        test_output = torch.cat((test_output, output))\n",
    "        test_target = torch.cat((test_target, targets))\n",
    "        loss = loss_function(output, targets) \n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "if torch.cuda.is_available():\n",
    "    test_output = test_output.cpu()\n",
    "    test_target = test_target.cpu()\n",
    "mse = test_loss / len(test_loader.dataset)\n",
    "rmse = mse ** 0.5\n",
    "print(\"MSE = \",mse)\n",
    "print(\"RMSE = \",rmse)\n",
    "\n",
    "mse_tensor = torch.tensor([mse,rmse])\n",
    "dir = \"./models/\"+ model_name +\"/results/\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "torch.save(mse_tensor, dir + \"mse_rmse.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_num = 200\n",
    "plt.plot(test_target[0:plot_num,:])\n",
    "plt.plot(test_output[0:plot_num,:])\n",
    "plt.legend([\"True external force\",\"Estimated external force\"],loc=\"upper right\")\n",
    "dir = \"./models/\"+ model_name +\"/results/\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "plt.savefig(dir+\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./models/\"+ model_name +\"/results/\"\n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)\n",
    "torch.save(test_target,dir + \"test_target.pt\")\n",
    "torch.save(test_output,dir + \"test_output.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVersion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
